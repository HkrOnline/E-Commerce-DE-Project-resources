from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, BooleanType, TimestampType, IntegerType, DecimalType
from datetime import datetime
import uuid

# Initialize Spark session
spark = SparkSession.builder.appName("EcommerceMetadataUpdate").getOrCreate()

# Define function to generate UUID
def generate_uuid():
    return str(uuid.uuid4())

# Define etl_table_config data
etl_table_config_data = [
    (generate_uuid(), "SQLServer", "Orders", "db.ecomm.orders", "Landing", "Incremental", "order_date", True, datetime.now(), datetime.now()),
    (generate_uuid(), "SQLServer", "OrderItems", "db.ecomm.order_items", "Landing", "Incremental", "order_date", True, datetime.now(), datetime.now()),
    (generate_uuid(), "SQLServer", "Payments", "db.ecomm.payments", "Landing", "Incremental", "payment_date", True, datetime.now(), datetime.now()),
    (generate_uuid(), "API", "Shipments", "api.logistics/shipments", "Landing", "Incremental", "shipment_date", True, datetime.now(), datetime.now()),
    (generate_uuid(), "SFTP", "Returns", "/sftp/returns/returns.csv", "Landing", "Incremental", "return_date", True, datetime.now(), datetime.now()),
    (generate_uuid(), "CSV", "Inventory", "/landing/inventory/inventory.csv", "Landing", "Full", "warehouse_id", True, datetime.now(), datetime.now()),
    (generate_uuid(), "API", "CustomerReviews", "api.reviews/customer_reviews", "Landing", "Incremental", "review_date", True, datetime.now(), datetime.now()),
    (generate_uuid(), "SQLServer", "Carts", "db.ecomm.carts", "Landing", "Incremental", "cart_date", True, datetime.now(), datetime.now()),
    (generate_uuid(), "CSV", "ProductPricingHistory", "/landing/pricing/pricing.csv", "Landing", "Incremental", "price_date", True, datetime.now(), datetime.now()),
    (generate_uuid(), "SQLServer", "Customers", "db.ecomm.customers", "Landing", "Full", "customer_id", True, datetime.now(), datetime.now()),
    (generate_uuid(), "SQLServer", "Products", "db.ecomm.products", "Landing", "Full", "product_id", True, datetime.now(), datetime.now()),
    (generate_uuid(), "CSV", "ProductAttributes", "/landing/attributes/attrs.csv", "Landing", "Full", "product_id", True, datetime.now(), datetime.now()),
    (generate_uuid(), "SQLServer", "Categories", "db.ecomm.categories", "Landing", "Full", "category_id", True, datetime.now(), datetime.now()),
    (generate_uuid(), "SFTP", "Vendors", "/sftp/vendors/vendors.csv", "Landing", "Full", "vendor_id", True, datetime.now(), datetime.now()),
    (generate_uuid(), "CSV", "Locations", "/landing/locations/locations.csv", "Landing", "Full", "location_id", True, datetime.now(), datetime.now()),
    (generate_uuid(), "SQLServer", "CustomerSegments", "db.ecomm.customer_segments", "Landing", "Full", "segment_id", True, datetime.now(), datetime.now())
]

# Write to etl_table_config
spark.createDataFrame(
    etl_table_config_data,
    ["table_id", "source_name", "table_name", "source_path", "target_layer", "load_type", "partition_column", "is_active", "created_at", "updated_at"]
).write.mode("append").saveAsTable("ecomm_metadata.etl_table_config")

# Define column_mapping data (example for Orders and Customers)
table_ids = spark.sql("SELECT table_id, table_name FROM ecomm_metadata.etl_table_config").collect()
orders_table_id = [row.table_id for row in table_ids if row.table_name == "Orders"][0]
customers_table_id = [row.table_id for row in table_ids if row.table_name == "Customers"][0]

column_mapping_data = [
    # Orders mappings
    (generate_uuid(), orders_table_id, "order_id", "order_id", "STRING", "TRIM(order_id)", False, datetime.now(), datetime.now()),
    (generate_uuid(), orders_table_id, "customer_id", "customer_id", "STRING", "TRIM(customer_id)", False, datetime.now(), datetime.now()),
    (generate_uuid(), orders_table_id, "order_date", "order_date", "TIMESTAMP", "CAST(order_date AS TIMESTAMP)", False, datetime.now(), datetime.now()),
    (generate_uuid(), orders_table_id, "order_total", "order_total", "DECIMAL(10,2)", "CAST(order_total AS DECIMAL(10,2))", False, datetime.now(), datetime.now()),
    (generate_uuid(), orders_table_id, "status", "status", "STRING", "TRIM(status)", True, datetime.now(), datetime.now()),
    # Customers mappings
    (generate_uuid(), customers_table_id, "customer_id", "customer_id", "STRING", "TRIM(customer_id)", False, datetime.now(), datetime.now()),
    (generate_uuid(), customers_table_id, "name", "name", "STRING", "TRIM(name)", True, datetime.now(), datetime.now()),
    (generate_uuid(), customers_table_id, "email", "email", "STRING", "TRIM(email)", True, datetime.now(), datetime.now()),
    (generate_uuid(), customers_table_id, "signup_date", "signup_date", "TIMESTAMP", "CAST(signup_date AS TIMESTAMP)", True, datetime.now(), datetime.now())
]

# Write to column_mapping
spark.createDataFrame(
    column_mapping_data,
    ["mapping_id", "table_id", "source_column", "target_column", "data_type", "transformation_rule", "is_nullable", "created_at", "updated_at"]
).write.mode("append").saveAsTable("ecomm_metadata.column_mapping")

# Optimize tables
spark.sql("OPTIMIZE ecomm_metadata.etl_table_config ZORDER BY (table_id)")
spark.sql("OPTIMIZE ecomm_metadata.column_mapping ZORDER BY (table_id, mapping_id)")

# Display for verification
spark.sql("SELECT * FROM ecomm_metadata.etl_table_config").show(truncate=False)
spark.sql("SELECT * FROM ecomm_metadata.column_mapping").show(truncate=False)
